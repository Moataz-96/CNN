{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.compat.v1.keras.backend import set_session\n",
    "\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n",
      "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
      "\n"
     ]
    }
   ],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True  # dynamically grow the memory used on the GPU\n",
    "config.log_device_placement = True  # to log device placement (on which device the operation ran)\n",
    "sess = tf.compat.v1.Session(config=config)\n",
    "set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 16055244706897441108\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 16350334588199943336\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 5419289824\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 12371992684721917474\n",
      "physical_device_desc: \"device: 0, name: GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 4313146249004533402\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data ,valid_data ,test_data = tfds.load('cifar100',split=['train[:90%]','train[90%:]','test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(item):\n",
    "    X = item['image']\n",
    "    y = item['coarse_label']\n",
    "    X = tf.image.resize_with_pad(X,224,224)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_data(data):\n",
    "    data = data.map(preprocessing)\n",
    "    data = data.shuffle(buffer_size = 100)\n",
    "    data = data.batch(32)\n",
    "    data = data.prefetch(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = configure_data(train_data)\n",
    "valid_data = configure_data(valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<tf.Tensor: shape=(64, 224, 224, 3), dtype=float32, numpy=\n",
      "array([[[[243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         ...,\n",
      "         [201., 217., 230.],\n",
      "         [201., 217., 230.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         ...,\n",
      "         [201., 217., 230.],\n",
      "         [201., 217., 230.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         [243., 243., 253.],\n",
      "         ...,\n",
      "         [201., 217., 230.],\n",
      "         [201., 217., 230.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[129., 116.,  94.],\n",
      "         [129., 116.,  94.],\n",
      "         [129., 116.,  94.],\n",
      "         ...,\n",
      "         [ 88.,  94.,  92.],\n",
      "         [ 88.,  94.,  92.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[129., 116.,  94.],\n",
      "         [129., 116.,  94.],\n",
      "         [129., 116.,  94.],\n",
      "         ...,\n",
      "         [ 88.,  94.,  92.],\n",
      "         [ 88.,  94.,  92.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]],\n",
      "\n",
      "\n",
      "       [[[166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         ...,\n",
      "         [108., 118.,  39.],\n",
      "         [108., 118.,  39.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         ...,\n",
      "         [108., 118.,  39.],\n",
      "         [108., 118.,  39.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         [166., 169., 116.],\n",
      "         ...,\n",
      "         [108., 118.,  39.],\n",
      "         [108., 118.,  39.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 88., 109.,  38.],\n",
      "         [ 88., 109.,  38.],\n",
      "         [ 88., 109.,  38.],\n",
      "         ...,\n",
      "         [ 45.,  49.,  15.],\n",
      "         [ 45.,  49.,  15.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[ 88., 109.,  38.],\n",
      "         [ 88., 109.,  38.],\n",
      "         [ 88., 109.,  38.],\n",
      "         ...,\n",
      "         [ 45.,  49.,  15.],\n",
      "         [ 45.,  49.,  15.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]],\n",
      "\n",
      "\n",
      "       [[[ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         ...,\n",
      "         [ 10., 140., 200.],\n",
      "         [ 10., 140., 200.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         ...,\n",
      "         [ 10., 140., 200.],\n",
      "         [ 10., 140., 200.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         [ 44., 142., 197.],\n",
      "         ...,\n",
      "         [ 10., 140., 200.],\n",
      "         [ 10., 140., 200.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  5.,  36.,  13.],\n",
      "         [  5.,  36.,  13.],\n",
      "         [  5.,  36.,  13.],\n",
      "         ...,\n",
      "         [ 20.,  44.,  18.],\n",
      "         [ 20.,  44.,  18.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  5.,  36.,  13.],\n",
      "         [  5.,  36.,  13.],\n",
      "         [  5.,  36.,  13.],\n",
      "         ...,\n",
      "         [ 20.,  44.,  18.],\n",
      "         [ 20.,  44.,  18.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         ...,\n",
      "         [ 99.,  99., 101.],\n",
      "         [ 99.,  99., 101.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         ...,\n",
      "         [ 99.,  99., 101.],\n",
      "         [ 99.,  99., 101.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         [129., 129., 129.],\n",
      "         ...,\n",
      "         [ 99.,  99., 101.],\n",
      "         [ 99.,  99., 101.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[129., 116.,  87.],\n",
      "         [129., 116.,  87.],\n",
      "         [129., 116.,  87.],\n",
      "         ...,\n",
      "         [106.,  88.,  46.],\n",
      "         [106.,  88.,  46.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[129., 116.,  87.],\n",
      "         [129., 116.,  87.],\n",
      "         [129., 116.,  87.],\n",
      "         ...,\n",
      "         [106.,  88.,  46.],\n",
      "         [106.,  88.,  46.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]],\n",
      "\n",
      "\n",
      "       [[[ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         ...,\n",
      "         [ 54.,  55.,  55.],\n",
      "         [ 54.,  55.,  55.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         ...,\n",
      "         [ 54.,  55.,  55.],\n",
      "         [ 54.,  55.,  55.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         [ 70.,  93.,  75.],\n",
      "         ...,\n",
      "         [ 54.,  55.,  55.],\n",
      "         [ 54.,  55.,  55.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[134., 150., 166.],\n",
      "         [134., 150., 166.],\n",
      "         [134., 150., 166.],\n",
      "         ...,\n",
      "         [ 88.,  89.,  87.],\n",
      "         [ 88.,  89.,  87.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[134., 150., 166.],\n",
      "         [134., 150., 166.],\n",
      "         [134., 150., 166.],\n",
      "         ...,\n",
      "         [ 88.,  89.,  87.],\n",
      "         [ 88.,  89.,  87.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]],\n",
      "\n",
      "\n",
      "       [[[173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         ...,\n",
      "         [157., 188., 213.],\n",
      "         [157., 188., 213.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         ...,\n",
      "         [157., 188., 213.],\n",
      "         [157., 188., 213.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         [173., 204., 223.],\n",
      "         ...,\n",
      "         [157., 188., 213.],\n",
      "         [157., 188., 213.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[117., 107., 109.],\n",
      "         [117., 107., 109.],\n",
      "         [117., 107., 109.],\n",
      "         ...,\n",
      "         [162., 153., 148.],\n",
      "         [162., 153., 148.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[117., 107., 109.],\n",
      "         [117., 107., 109.],\n",
      "         [117., 107., 109.],\n",
      "         ...,\n",
      "         [162., 153., 148.],\n",
      "         [162., 153., 148.],\n",
      "         [  0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.]]]], dtype=float32)>, <tf.Tensor: shape=(64,), dtype=int64, numpy=\n",
      "array([ 9,  2, 10,  3,  4, 19, 17,  6, 15, 14,  8, 11,  1,  1, 14,  5, 10,\n",
      "        4, 10, 18,  1, 16, 16, 18,  8, 17, 11,  2, 18,  8,  0,  0,  6, 15,\n",
      "       17,  0,  9, 11,  4,  9, 17,  0, 19, 12,  8, 12, 19, 19, 15,  9, 17,\n",
      "        5, 10,  4, 16,  0,  9, 17,  7, 13,  8,  8, 11, 19])>)\n"
     ]
    }
   ],
   "source": [
    "for item in train_data.take(1):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "In (ZeroPadding2D)           (None, 36, 36, 3)         0         \n",
      "_________________________________________________________________\n",
      "C1 (Conv2D)                  (None, 32, 32, 6)         456       \n",
      "_________________________________________________________________\n",
      "S2 (AveragePooling2D)        (None, 16, 16, 6)         0         \n",
      "_________________________________________________________________\n",
      "C3 (Conv2D)                  (None, 12, 12, 16)        2416      \n",
      "_________________________________________________________________\n",
      "S4 (AveragePooling2D)        (None, 6, 6, 16)          0         \n",
      "_________________________________________________________________\n",
      "C5 (Conv2D)                  (None, 2, 2, 120)         48120     \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 480)               0         \n",
      "_________________________________________________________________\n",
      "F6 (Dense)                   (None, 84)                40404     \n",
      "_________________________________________________________________\n",
      "Out (Dense)                  (None, 100)               8500      \n",
      "=================================================================\n",
      "Total params: 99,896\n",
      "Trainable params: 99,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import CNN_Models.LeNet5 as LeNet5\n",
    "model_lenet = LeNet5.lenet_5(in_shape=[32, 32, 3], n_classes=100, opt='sgd')\n",
    "model_lenet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "2813/2813 [==============================] - 9s 3ms/step - loss: 2.8978 - accuracy: 0.1087 - val_loss: 2.7751 - val_accuracy: 0.1268\n",
      "Epoch 2/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 2.7086 - accuracy: 0.1595 - val_loss: 2.6666 - val_accuracy: 0.1716\n",
      "Epoch 3/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.6950 - accuracy: 0.1670 - val_loss: 2.6862 - val_accuracy: 0.1756\n",
      "Epoch 4/30\n",
      "2813/2813 [==============================] - 7s 3ms/step - loss: 2.5994 - accuracy: 0.2031 - val_loss: 2.5748 - val_accuracy: 0.2082\n",
      "Epoch 5/30\n",
      "2813/2813 [==============================] - 7s 3ms/step - loss: 2.5059 - accuracy: 0.2318 - val_loss: 2.4906 - val_accuracy: 0.2344\n",
      "Epoch 6/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.4356 - accuracy: 0.2530 - val_loss: 2.4302 - val_accuracy: 0.2588\n",
      "Epoch 7/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.3713 - accuracy: 0.2764 - val_loss: 2.4290 - val_accuracy: 0.2592\n",
      "Epoch 8/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.3562 - accuracy: 0.2770 - val_loss: 2.3861 - val_accuracy: 0.2690\n",
      "Epoch 9/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.2819 - accuracy: 0.3005 - val_loss: 2.3948 - val_accuracy: 0.2746\n",
      "Epoch 10/30\n",
      "2813/2813 [==============================] - 5s 2ms/step - loss: 2.2285 - accuracy: 0.3172 - val_loss: 2.2885 - val_accuracy: 0.3032\n",
      "Epoch 11/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.1892 - accuracy: 0.3298 - val_loss: 2.2480 - val_accuracy: 0.3156\n",
      "Epoch 12/30\n",
      "2813/2813 [==============================] - 5s 2ms/step - loss: 2.1503 - accuracy: 0.3398 - val_loss: 2.2642 - val_accuracy: 0.3100\n",
      "Epoch 13/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.1116 - accuracy: 0.3509 - val_loss: 2.2155 - val_accuracy: 0.3226\n",
      "Epoch 14/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.0790 - accuracy: 0.3613 - val_loss: 2.2158 - val_accuracy: 0.3234\n",
      "Epoch 15/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.0455 - accuracy: 0.3707 - val_loss: 2.1815 - val_accuracy: 0.3364\n",
      "Epoch 16/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 2.0210 - accuracy: 0.3771 - val_loss: 2.2102 - val_accuracy: 0.3386\n",
      "Epoch 17/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.9912 - accuracy: 0.3837 - val_loss: 2.2107 - val_accuracy: 0.3340\n",
      "Epoch 18/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.9664 - accuracy: 0.3915 - val_loss: 2.1474 - val_accuracy: 0.3506\n",
      "Epoch 19/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 1.9370 - accuracy: 0.4018 - val_loss: 2.1892 - val_accuracy: 0.3372\n",
      "Epoch 20/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 1.9226 - accuracy: 0.4068 - val_loss: 2.1787 - val_accuracy: 0.3404\n",
      "Epoch 21/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 1.8937 - accuracy: 0.4129 - val_loss: 2.2017 - val_accuracy: 0.3406\n",
      "Epoch 22/30\n",
      "2813/2813 [==============================] - 7s 3ms/step - loss: 1.8714 - accuracy: 0.4187 - val_loss: 2.1795 - val_accuracy: 0.3524\n",
      "Epoch 23/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 1.8438 - accuracy: 0.4261 - val_loss: 2.1790 - val_accuracy: 0.3470\n",
      "Epoch 24/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.8265 - accuracy: 0.4316 - val_loss: 2.1299 - val_accuracy: 0.3600\n",
      "Epoch 25/30\n",
      "2813/2813 [==============================] - 7s 2ms/step - loss: 1.8025 - accuracy: 0.4407 - val_loss: 2.1680 - val_accuracy: 0.3484\n",
      "Epoch 26/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.7777 - accuracy: 0.4470 - val_loss: 2.1601 - val_accuracy: 0.3490\n",
      "Epoch 27/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.7569 - accuracy: 0.4553 - val_loss: 2.1722 - val_accuracy: 0.3534\n",
      "Epoch 28/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.7326 - accuracy: 0.4609 - val_loss: 2.2025 - val_accuracy: 0.3432\n",
      "Epoch 29/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.7165 - accuracy: 0.4668 - val_loss: 2.2215 - val_accuracy: 0.3438\n",
      "Epoch 30/30\n",
      "2813/2813 [==============================] - 6s 2ms/step - loss: 1.7069 - accuracy: 0.4697 - val_loss: 2.2291 - val_accuracy: 0.3500\n"
     ]
    }
   ],
   "source": [
    "history_lenet = model_lenet.fit(train_data, epochs=30, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "1_7x7_conv_64_2 (Conv2D)        (None, 112, 112, 64) 9472        input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "1_pool/2 (MaxPooling2D)         (None, 56, 56, 64)   0           1_7x7_conv_64_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "2_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       1_pool/2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "2_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         2_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "2_relu (Activation)             (None, 56, 56, 64)   0           2_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       2_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "3_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         3_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "3_add (Add)                     (None, 56, 56, 64)   0           1_pool/2[0][0]                   \n",
      "                                                                 3_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3_relu (Activation)             (None, 56, 56, 64)   0           3_add[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "4_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       3_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "4_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         4_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "4_relu (Activation)             (None, 56, 56, 64)   0           4_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "5_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       4_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "5_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         5_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "5_add (Add)                     (None, 56, 56, 64)   0           3_relu[0][0]                     \n",
      "                                                                 5_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "5_relu (Activation)             (None, 56, 56, 64)   0           5_add[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "6_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       5_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "6_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         6_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "6_relu (Activation)             (None, 56, 56, 64)   0           6_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "7_main_3x3_conv_64_1 (Conv2D)   (None, 56, 56, 64)   36928       6_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "7_batch_norm (BatchNormalizatio (None, 56, 56, 64)   256         7_main_3x3_conv_64_1[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "7_add (Add)                     (None, 56, 56, 64)   0           5_relu[0][0]                     \n",
      "                                                                 7_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "7_relu (Activation)             (None, 56, 56, 64)   0           7_add[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "8_main_3x3_conv_128_2 (Conv2D)  (None, 28, 28, 128)  73856       7_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "8_batch_norm (BatchNormalizatio (None, 28, 28, 128)  512         8_main_3x3_conv_128_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "8_relu (Activation)             (None, 28, 28, 128)  0           8_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "9_skip_conv_1x1 (Conv2D)        (None, 28, 28, 128)  8320        7_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "9_main_3x3_conv_128_2 (Conv2D)  (None, 28, 28, 128)  147584      8_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "9_skip_batch_norm (BatchNormali (None, 28, 28, 128)  512         9_skip_conv_1x1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "9_batch_norm (BatchNormalizatio (None, 28, 28, 128)  512         9_main_3x3_conv_128_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "9_add (Add)                     (None, 28, 28, 128)  0           9_skip_batch_norm[0][0]          \n",
      "                                                                 9_batch_norm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "9_relu (Activation)             (None, 28, 28, 128)  0           9_add[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "10_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      9_relu[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "10_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         10_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "10_relu (Activation)            (None, 28, 28, 128)  0           10_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "11_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      10_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "11_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         11_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "11_add (Add)                    (None, 28, 28, 128)  0           9_relu[0][0]                     \n",
      "                                                                 11_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "11_relu (Activation)            (None, 28, 28, 128)  0           11_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "12_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      11_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "12_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         12_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "12_relu (Activation)            (None, 28, 28, 128)  0           12_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "13_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      12_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "13_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         13_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "13_add (Add)                    (None, 28, 28, 128)  0           11_relu[0][0]                    \n",
      "                                                                 13_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "13_relu (Activation)            (None, 28, 28, 128)  0           13_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "14_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      13_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "14_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         14_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "14_relu (Activation)            (None, 28, 28, 128)  0           14_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "15_main_3x3_conv_128_1 (Conv2D) (None, 28, 28, 128)  147584      14_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "15_batch_norm (BatchNormalizati (None, 28, 28, 128)  512         15_main_3x3_conv_128_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "15_add (Add)                    (None, 28, 28, 128)  0           13_relu[0][0]                    \n",
      "                                                                 15_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "15_relu (Activation)            (None, 28, 28, 128)  0           15_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "16_main_3x3_conv_256_2 (Conv2D) (None, 14, 14, 256)  295168      15_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "16_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        16_main_3x3_conv_256_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "16_relu (Activation)            (None, 14, 14, 256)  0           16_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "17_skip_conv_1x1 (Conv2D)       (None, 14, 14, 256)  33024       15_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "17_main_3x3_conv_256_2 (Conv2D) (None, 14, 14, 256)  590080      16_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "17_skip_batch_norm (BatchNormal (None, 14, 14, 256)  1024        17_skip_conv_1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "17_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        17_main_3x3_conv_256_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "17_add (Add)                    (None, 14, 14, 256)  0           17_skip_batch_norm[0][0]         \n",
      "                                                                 17_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "17_relu (Activation)            (None, 14, 14, 256)  0           17_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "18_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      17_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "18_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        18_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "18_relu (Activation)            (None, 14, 14, 256)  0           18_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "19_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      18_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "19_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        19_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "19_add (Add)                    (None, 14, 14, 256)  0           17_relu[0][0]                    \n",
      "                                                                 19_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "19_relu (Activation)            (None, 14, 14, 256)  0           19_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "20_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      19_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "20_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        20_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "20_relu (Activation)            (None, 14, 14, 256)  0           20_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "21_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      20_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "21_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        21_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "21_add (Add)                    (None, 14, 14, 256)  0           19_relu[0][0]                    \n",
      "                                                                 21_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "21_relu (Activation)            (None, 14, 14, 256)  0           21_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "22_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      21_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "22_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        22_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "22_relu (Activation)            (None, 14, 14, 256)  0           22_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "23_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      22_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "23_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        23_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "23_add (Add)                    (None, 14, 14, 256)  0           21_relu[0][0]                    \n",
      "                                                                 23_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "23_relu (Activation)            (None, 14, 14, 256)  0           23_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "24_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      23_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "24_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        24_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "24_relu (Activation)            (None, 14, 14, 256)  0           24_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "25_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      24_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "25_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        25_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "25_add (Add)                    (None, 14, 14, 256)  0           23_relu[0][0]                    \n",
      "                                                                 25_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "25_relu (Activation)            (None, 14, 14, 256)  0           25_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "26_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      25_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "26_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        26_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "26_relu (Activation)            (None, 14, 14, 256)  0           26_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "27_main_3x3_conv_256_1 (Conv2D) (None, 14, 14, 256)  590080      26_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "27_batch_norm (BatchNormalizati (None, 14, 14, 256)  1024        27_main_3x3_conv_256_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "27_add (Add)                    (None, 14, 14, 256)  0           25_relu[0][0]                    \n",
      "                                                                 27_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "27_relu (Activation)            (None, 14, 14, 256)  0           27_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "28_main_3x3_conv_512_2 (Conv2D) (None, 7, 7, 512)    1180160     27_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "28_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        28_main_3x3_conv_512_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "28_relu (Activation)            (None, 7, 7, 512)    0           28_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "29_skip_conv_1x1 (Conv2D)       (None, 7, 7, 512)    131584      27_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "29_main_3x3_conv_512_2 (Conv2D) (None, 7, 7, 512)    2359808     28_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "29_skip_batch_norm (BatchNormal (None, 7, 7, 512)    2048        29_skip_conv_1x1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "29_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        29_main_3x3_conv_512_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "29_add (Add)                    (None, 7, 7, 512)    0           29_skip_batch_norm[0][0]         \n",
      "                                                                 29_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "29_relu (Activation)            (None, 7, 7, 512)    0           29_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "30_main_3x3_conv_512_1 (Conv2D) (None, 7, 7, 512)    2359808     29_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "30_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        30_main_3x3_conv_512_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "30_relu (Activation)            (None, 7, 7, 512)    0           30_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "31_main_3x3_conv_512_1 (Conv2D) (None, 7, 7, 512)    2359808     30_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "31_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        31_main_3x3_conv_512_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "31_add (Add)                    (None, 7, 7, 512)    0           29_relu[0][0]                    \n",
      "                                                                 31_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "31_relu (Activation)            (None, 7, 7, 512)    0           31_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "32_main_3x3_conv_512_1 (Conv2D) (None, 7, 7, 512)    2359808     31_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "32_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        32_main_3x3_conv_512_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "32_relu (Activation)            (None, 7, 7, 512)    0           32_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "33_main_3x3_conv_512_1 (Conv2D) (None, 7, 7, 512)    2359808     32_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "33_batch_norm (BatchNormalizati (None, 7, 7, 512)    2048        33_main_3x3_conv_512_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "33_add (Add)                    (None, 7, 7, 512)    0           31_relu[0][0]                    \n",
      "                                                                 33_batch_norm[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "33_relu (Activation)            (None, 7, 7, 512)    0           33_add[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "34_global_avg_pool_ (GlobalAver (None, 512)          0           33_relu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 100)          51300       34_global_avg_pool_[0][0]        \n",
      "==================================================================================================\n",
      "Total params: 21,361,252\n",
      "Trainable params: 21,344,356\n",
      "Non-trainable params: 16,896\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import CNN_Models.ResNet34 as ResNet34\n",
    "model_resnet34 = ResNet34.ResNet34(in_shape=[224,224,3], n_classes=100, opt='Adam')\n",
    "model_resnet34.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "704/704 [==============================] - 213s 303ms/step - loss: 2.5160 - accuracy: 0.2325 - val_loss: 3.7282 - val_accuracy: 0.1550\n",
      "Epoch 2/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 2.0513 - accuracy: 0.3639 - val_loss: 2.2767 - val_accuracy: 0.3434\n",
      "Epoch 3/30\n",
      "704/704 [==============================] - 212s 301ms/step - loss: 1.6715 - accuracy: 0.4710 - val_loss: 1.8282 - val_accuracy: 0.4372\n",
      "Epoch 4/30\n",
      "704/704 [==============================] - 212s 301ms/step - loss: 1.3928 - accuracy: 0.5546 - val_loss: 1.8853 - val_accuracy: 0.4914\n",
      "Epoch 5/30\n",
      "704/704 [==============================] - 212s 301ms/step - loss: 1.1731 - accuracy: 0.6201 - val_loss: 1.4565 - val_accuracy: 0.5430\n",
      "Epoch 6/30\n",
      "704/704 [==============================] - 208s 295ms/step - loss: 0.9889 - accuracy: 0.6788 - val_loss: 1.7372 - val_accuracy: 0.5660\n",
      "Epoch 7/30\n",
      "704/704 [==============================] - 207s 295ms/step - loss: 0.8132 - accuracy: 0.7337 - val_loss: 1.4916 - val_accuracy: 0.5796\n",
      "Epoch 8/30\n",
      "704/704 [==============================] - 209s 297ms/step - loss: 0.6440 - accuracy: 0.7853 - val_loss: 1.5870 - val_accuracy: 0.5750\n",
      "Epoch 9/30\n",
      "704/704 [==============================] - 211s 299ms/step - loss: 0.4690 - accuracy: 0.8421 - val_loss: 1.9651 - val_accuracy: 0.5526\n",
      "Epoch 10/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 0.3342 - accuracy: 0.8881 - val_loss: 2.0141 - val_accuracy: 0.5678\n",
      "Epoch 11/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 0.2346 - accuracy: 0.9207 - val_loss: 1.8750 - val_accuracy: 0.6038\n",
      "Epoch 12/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 0.1741 - accuracy: 0.9404 - val_loss: 1.9629 - val_accuracy: 0.6266\n",
      "Epoch 13/30\n",
      "704/704 [==============================] - 210s 298ms/step - loss: 0.1616 - accuracy: 0.9469 - val_loss: 2.1359 - val_accuracy: 0.6174\n",
      "Epoch 14/30\n",
      "704/704 [==============================] - 208s 296ms/step - loss: 0.1134 - accuracy: 0.9613 - val_loss: 2.3476 - val_accuracy: 0.5972\n",
      "Epoch 15/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 0.1192 - accuracy: 0.9594 - val_loss: 2.7110 - val_accuracy: 0.5740\n",
      "Epoch 16/30\n",
      "704/704 [==============================] - 212s 300ms/step - loss: 0.0986 - accuracy: 0.9663 - val_loss: 2.1327 - val_accuracy: 0.6276\n",
      "Epoch 17/30\n",
      "704/704 [==============================] - 211s 300ms/step - loss: 0.0949 - accuracy: 0.9691 - val_loss: 2.0293 - val_accuracy: 0.6454\n",
      "Epoch 18/30\n",
      "302/704 [===========>..................] - ETA: 2:02 - loss: 0.1042 - accuracy: 0.9665"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c13e9d9fe28b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory_resnet34\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_resnet34\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history_resnet34 = model_resnet34.fit(train_data, epochs=30, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "Input (InputLayer)              [(None, 224, 224, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv_1_7x7/2 (Conv2D)           (None, 112, 112, 64) 9472        Input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_1_3x3/2 (MaxPooling2D) (None, 56, 56, 64)   0           conv_1_7x7/2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_3x3/2_reduced (Conv2D)   (None, 56, 56, 64)   4160        max_pool_1_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv_2_3x3/2 (Conv2D)           (None, 56, 56, 192)  110784      conv_2_3x3/2_reduced[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_2_3x3/2 (MaxPooling2D) (None, 28, 28, 192)  0           conv_2_3x3/2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_3x3_reduced (Conv2D)    (None, 28, 28, 96)   18528       max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_5x5_reduced (Conv2D)    (None, 28, 28, 16)   3088        max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "3a_max_pool_3x3 (MaxPooling2D)  (None, 28, 28, 192)  0           max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_1x1 (Conv2D)            (None, 28, 28, 64)   12352       max_pool_2_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_3x3 (Conv2D)            (None, 28, 28, 128)  110720      3a_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_5x5 (Conv2D)            (None, 28, 28, 32)   12832       3a_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "3a_conv_1x1_pool_proj (Conv2D)  (None, 28, 28, 32)   6176        3a_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3a (Concatenate)                (None, 28, 28, 256)  0           3a_conv_1x1[0][0]                \n",
      "                                                                 3a_conv_3x3[0][0]                \n",
      "                                                                 3a_conv_5x5[0][0]                \n",
      "                                                                 3a_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_3x3_reduced (Conv2D)    (None, 28, 28, 128)  32896       3a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_5x5_reduced (Conv2D)    (None, 28, 28, 32)   8224        3a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "3b_max_pool_3x3 (MaxPooling2D)  (None, 28, 28, 256)  0           3a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_1x1 (Conv2D)            (None, 28, 28, 128)  32896       3a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_3x3 (Conv2D)            (None, 28, 28, 192)  221376      3b_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_5x5 (Conv2D)            (None, 28, 28, 96)   76896       3b_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "3b_conv_1x1_pool_proj (Conv2D)  (None, 28, 28, 64)   16448       3b_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "3b (Concatenate)                (None, 28, 28, 480)  0           3b_conv_1x1[0][0]                \n",
      "                                                                 3b_conv_3x3[0][0]                \n",
      "                                                                 3b_conv_5x5[0][0]                \n",
      "                                                                 3b_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_3_3x3/2 (MaxPooling2D) (None, 14, 14, 480)  0           3b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_3x3_reduced (Conv2D)    (None, 14, 14, 96)   46176       max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_5x5_reduced (Conv2D)    (None, 14, 14, 16)   7696        max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "4a_max_pool_3x3 (MaxPooling2D)  (None, 14, 14, 480)  0           max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_1x1 (Conv2D)            (None, 14, 14, 192)  92352       max_pool_3_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_3x3 (Conv2D)            (None, 14, 14, 208)  179920      4a_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_5x5 (Conv2D)            (None, 14, 14, 48)   19248       4a_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4a_conv_1x1_pool_proj (Conv2D)  (None, 14, 14, 64)   30784       4a_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "4a (Concatenate)                (None, 14, 14, 512)  0           4a_conv_1x1[0][0]                \n",
      "                                                                 4a_conv_3x3[0][0]                \n",
      "                                                                 4a_conv_5x5[0][0]                \n",
      "                                                                 4a_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_3x3_reduced (Conv2D)    (None, 14, 14, 112)  57456       4a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_5x5_reduced (Conv2D)    (None, 14, 14, 24)   12312       4a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4b_max_pool_3x3 (MaxPooling2D)  (None, 14, 14, 512)  0           4a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_1x1 (Conv2D)            (None, 14, 14, 160)  82080       4a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_3x3 (Conv2D)            (None, 14, 14, 224)  226016      4b_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_5x5 (Conv2D)            (None, 14, 14, 64)   38464       4b_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4b_conv_1x1_pool_proj (Conv2D)  (None, 14, 14, 64)   32832       4b_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "4b (Concatenate)                (None, 14, 14, 512)  0           4b_conv_1x1[0][0]                \n",
      "                                                                 4b_conv_3x3[0][0]                \n",
      "                                                                 4b_conv_5x5[0][0]                \n",
      "                                                                 4b_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_3x3_reduced (Conv2D)    (None, 14, 14, 128)  65664       4b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_5x5_reduced (Conv2D)    (None, 14, 14, 24)   12312       4b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4c_max_pool_3x3 (MaxPooling2D)  (None, 14, 14, 512)  0           4b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_1x1 (Conv2D)            (None, 14, 14, 128)  65664       4b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_3x3 (Conv2D)            (None, 14, 14, 256)  295168      4c_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_5x5 (Conv2D)            (None, 14, 14, 64)   38464       4c_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4c_conv_1x1_pool_proj (Conv2D)  (None, 14, 14, 64)   32832       4c_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "4c (Concatenate)                (None, 14, 14, 512)  0           4c_conv_1x1[0][0]                \n",
      "                                                                 4c_conv_3x3[0][0]                \n",
      "                                                                 4c_conv_5x5[0][0]                \n",
      "                                                                 4c_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_3x3_reduced (Conv2D)    (None, 14, 14, 144)  73872       4c[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_5x5_reduced (Conv2D)    (None, 14, 14, 32)   16416       4c[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4d_max_pool_3x3 (MaxPooling2D)  (None, 14, 14, 512)  0           4c[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_1x1 (Conv2D)            (None, 14, 14, 112)  57456       4c[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_3x3 (Conv2D)            (None, 14, 14, 288)  373536      4d_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_5x5 (Conv2D)            (None, 14, 14, 64)   51264       4d_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4d_conv_1x1_pool_proj (Conv2D)  (None, 14, 14, 64)   32832       4d_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "4d (Concatenate)                (None, 14, 14, 528)  0           4d_conv_1x1[0][0]                \n",
      "                                                                 4d_conv_3x3[0][0]                \n",
      "                                                                 4d_conv_5x5[0][0]                \n",
      "                                                                 4d_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_3x3_reduced (Conv2D)    (None, 14, 14, 160)  84640       4d[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_5x5_reduced (Conv2D)    (None, 14, 14, 32)   16928       4d[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4e_max_pool_3x3 (MaxPooling2D)  (None, 14, 14, 528)  0           4d[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_1x1 (Conv2D)            (None, 14, 14, 256)  135424      4d[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_3x3 (Conv2D)            (None, 14, 14, 320)  461120      4e_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_5x5 (Conv2D)            (None, 14, 14, 128)  102528      4e_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "4e_conv_1x1_pool_proj (Conv2D)  (None, 14, 14, 128)  67712       4e_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "4e (Concatenate)                (None, 14, 14, 832)  0           4e_conv_1x1[0][0]                \n",
      "                                                                 4e_conv_3x3[0][0]                \n",
      "                                                                 4e_conv_5x5[0][0]                \n",
      "                                                                 4e_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pool_4_3x3/2 (MaxPooling2D) (None, 7, 7, 832)    0           4e[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_3x3_reduced (Conv2D)    (None, 7, 7, 160)    133280      max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_5x5_reduced (Conv2D)    (None, 7, 7, 32)     26656       max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "5a_max_pool_3x3 (MaxPooling2D)  (None, 7, 7, 832)    0           max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_1x1 (Conv2D)            (None, 7, 7, 256)    213248      max_pool_4_3x3/2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_3x3 (Conv2D)            (None, 7, 7, 320)    461120      5a_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_5x5 (Conv2D)            (None, 7, 7, 128)    102528      5a_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "5a_conv_1x1_pool_proj (Conv2D)  (None, 7, 7, 128)    106624      5a_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "5a (Concatenate)                (None, 7, 7, 832)    0           5a_conv_1x1[0][0]                \n",
      "                                                                 5a_conv_3x3[0][0]                \n",
      "                                                                 5a_conv_5x5[0][0]                \n",
      "                                                                 5a_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_3x3_reduced (Conv2D)    (None, 7, 7, 192)    159936      5a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_5x5_reduced (Conv2D)    (None, 7, 7, 84)     69972       5a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "5b_max_pool_3x3 (MaxPooling2D)  (None, 7, 7, 832)    0           5a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary1_avg_pooling_5x5/3 (A (None, 4, 4, 512)    0           4a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary2_avg_pooling_5x5/3 (A (None, 4, 4, 528)    0           4d[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_1x1 (Conv2D)            (None, 7, 7, 384)    319872      5a[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_3x3 (Conv2D)            (None, 7, 7, 384)    663936      5b_conv_3x3_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_5x5 (Conv2D)            (None, 7, 7, 128)    268928      5b_conv_5x5_reduced[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "5b_conv_1x1_pool_proj (Conv2D)  (None, 7, 7, 128)    106624      5b_max_pool_3x3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary1_conv_1x1 (Conv2D)    (None, 4, 4, 128)    65664       auxiliary1_avg_pooling_5x5/3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "auxiliary2_conv_1x1 (Conv2D)    (None, 4, 4, 128)    67712       auxiliary2_avg_pooling_5x5/3[0][0\n",
      "__________________________________________________________________________________________________\n",
      "5b (Concatenate)                (None, 7, 7, 1024)   0           5b_conv_1x1[0][0]                \n",
      "                                                                 5b_conv_3x3[0][0]                \n",
      "                                                                 5b_conv_5x5[0][0]                \n",
      "                                                                 5b_conv_1x1_pool_proj[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 2048)         0           auxiliary1_conv_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 2048)         0           auxiliary2_conv_1x1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_avg_pool_5_7x7/1 (Global (None, 1024)         0           5b[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary1_fc_1024 (Dense)      (None, 1024)         2098176     flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary2_fc_1024 (Dense)      (None, 1024)         2098176     flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_output (Dropout)        (None, 1024)         0           global_avg_pool_5_7x7/1[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1024)         0           auxiliary1_fc_1024[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 1024)         0           auxiliary2_fc_1024[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "softmax_output (Dense)          (None, 100)          102500      dropout_output[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary1_output (Dense)       (None, 100)          102500      dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "auxiliary2_output (Dense)       (None, 100)          102500      dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 10,755,968\n",
      "Trainable params: 10,755,968\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import CNN_Models.Inception_v1 as Inception_v1\n",
    "model_inception = Inception_v1.Inception(in_shape=[224,224,3], n_classes=100, opt='adam')\n",
    "model_inception.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "704/704 [==============================] - 150s 213ms/step - loss: 2.7629 - softmax_output_loss: 2.7625 - auxiliary1_output_loss: 2.7612 - auxiliary2_output_loss: 2.7650 - softmax_output_accuracy: 0.1334 - auxiliary1_output_accuracy: 0.1603 - auxiliary2_output_accuracy: 0.1444 - val_loss: 2.4460 - val_softmax_output_loss: 2.4802 - val_auxiliary1_output_loss: 2.4042 - val_auxiliary2_output_loss: 2.4421 - val_softmax_output_accuracy: 0.2146 - val_auxiliary1_output_accuracy: 0.2568 - val_auxiliary2_output_accuracy: 0.2400\n",
      "Epoch 2/10\n",
      "704/704 [==============================] - 151s 215ms/step - loss: 2.3648 - softmax_output_loss: 2.3894 - auxiliary1_output_loss: 2.3343 - auxiliary2_output_loss: 2.3625 - softmax_output_accuracy: 0.2516 - auxiliary1_output_accuracy: 0.2774 - auxiliary2_output_accuracy: 0.2659 - val_loss: 2.2229 - val_softmax_output_loss: 2.3068 - val_auxiliary1_output_loss: 2.1380 - val_auxiliary2_output_loss: 2.1960 - val_softmax_output_accuracy: 0.2778 - val_auxiliary1_output_accuracy: 0.3500 - val_auxiliary2_output_accuracy: 0.3256\n",
      "Epoch 3/10\n",
      "704/704 [==============================] - 149s 212ms/step - loss: 2.1434 - softmax_output_loss: 2.1636 - auxiliary1_output_loss: 2.1163 - auxiliary2_output_loss: 2.1435 - softmax_output_accuracy: 0.3209 - auxiliary1_output_accuracy: 0.3436 - auxiliary2_output_accuracy: 0.3306 - val_loss: 2.0543 - val_softmax_output_loss: 2.1236 - val_auxiliary1_output_loss: 1.9758 - val_auxiliary2_output_loss: 2.0403 - val_softmax_output_accuracy: 0.3378 - val_auxiliary1_output_accuracy: 0.3948 - val_auxiliary2_output_accuracy: 0.3610- auxiliary1_output_loss: 2.1172 - auxiliary2_output_loss: 2.1449 - softmax_output_accuracy: 0.3205 - auxiliary1_output_accuracy: 0.3432 - auxiliary2_output\n",
      "Epoch 4/10\n",
      "704/704 [==============================] - 146s 208ms/step - loss: 1.9716 - softmax_output_loss: 1.9696 - auxiliary1_output_loss: 1.9666 - auxiliary2_output_loss: 1.9792 - softmax_output_accuracy: 0.3814 - auxiliary1_output_accuracy: 0.3883 - auxiliary2_output_accuracy: 0.3839 - val_loss: 1.8699 - val_softmax_output_loss: 1.8977 - val_auxiliary1_output_loss: 1.8456 - val_auxiliary2_output_loss: 1.8571 - val_softmax_output_accuracy: 0.4024 - val_auxiliary1_output_accuracy: 0.4340 - val_auxiliary2_output_accuracy: 0.4192\n",
      "Epoch 5/10\n",
      "704/704 [==============================] - 146s 208ms/step - loss: 1.8299 - softmax_output_loss: 1.8145 - auxiliary1_output_loss: 1.8459 - auxiliary2_output_loss: 1.8345 - softmax_output_accuracy: 0.4276 - auxiliary1_output_accuracy: 0.4247 - auxiliary2_output_accuracy: 0.4257 - val_loss: 1.7594 - val_softmax_output_loss: 1.7845 - val_auxiliary1_output_loss: 1.7522 - val_auxiliary2_output_loss: 1.7330 - val_softmax_output_accuracy: 0.4396 - val_auxiliary1_output_accuracy: 0.4568 - val_auxiliary2_output_accuracy: 0.4548\n",
      "Epoch 6/10\n",
      "704/704 [==============================] - 146s 208ms/step - loss: 1.7009 - softmax_output_loss: 1.6688 - auxiliary1_output_loss: 1.7378 - auxiliary2_output_loss: 1.7066 - softmax_output_accuracy: 0.4697 - auxiliary1_output_accuracy: 0.4548 - auxiliary2_output_accuracy: 0.4633 - val_loss: 1.6897 - val_softmax_output_loss: 1.6738 - val_auxiliary1_output_loss: 1.7115 - val_auxiliary2_output_loss: 1.6893 - val_softmax_output_accuracy: 0.4708 - val_auxiliary1_output_accuracy: 0.4746 - val_auxiliary2_output_accuracy: 0.4760\n",
      "Epoch 7/10\n",
      "704/704 [==============================] - 146s 208ms/step - loss: 1.5830 - softmax_output_loss: 1.5379 - auxiliary1_output_loss: 1.6393 - auxiliary2_output_loss: 1.5868 - softmax_output_accuracy: 0.5112 - auxiliary1_output_accuracy: 0.4860 - auxiliary2_output_accuracy: 0.4989 - val_loss: 1.5671 - val_softmax_output_loss: 1.5553 - val_auxiliary1_output_loss: 1.5993 - val_auxiliary2_output_loss: 1.5506 - val_softmax_output_accuracy: 0.5148 - val_auxiliary1_output_accuracy: 0.5014 - val_auxiliary2_output_accuracy: 0.5040\n",
      "Epoch 8/10\n",
      "704/704 [==============================] - 146s 207ms/step - loss: 1.4891 - softmax_output_loss: 1.4353 - auxiliary1_output_loss: 1.5638 - auxiliary2_output_loss: 1.4859 - softmax_output_accuracy: 0.5417 - auxiliary1_output_accuracy: 0.5071 - auxiliary2_output_accuracy: 0.5306 - val_loss: 1.5573 - val_softmax_output_loss: 1.5699 - val_auxiliary1_output_loss: 1.5677 - val_auxiliary2_output_loss: 1.5301 - val_softmax_output_accuracy: 0.5164 - val_auxiliary1_output_accuracy: 0.5160 - val_auxiliary2_output_accuracy: 0.5252max_output_accuracy: 0.5414 - auxiliary1_output_accuracy: 0.5067 - aux\n",
      "Epoch 9/10\n",
      "704/704 [==============================] - 146s 207ms/step - loss: 1.3978 - softmax_output_loss: 1.3343 - auxiliary1_output_loss: 1.4821 - auxiliary2_output_loss: 1.3980 - softmax_output_accuracy: 0.5702 - auxiliary1_output_accuracy: 0.5285 - auxiliary2_output_accuracy: 0.5542 - val_loss: 1.5045 - val_softmax_output_loss: 1.4910 - val_auxiliary1_output_loss: 1.5382 - val_auxiliary2_output_loss: 1.4888 - val_softmax_output_accuracy: 0.5294 - val_auxiliary1_output_accuracy: 0.5218 - val_auxiliary2_output_accuracy: 0.5242\n",
      "Epoch 10/10\n",
      "704/704 [==============================] - 146s 207ms/step - loss: 1.3145 - softmax_output_loss: 1.2390 - auxiliary1_output_loss: 1.4233 - auxiliary2_output_loss: 1.3063 - softmax_output_accuracy: 0.6017 - auxiliary1_output_accuracy: 0.5477 - auxiliary2_output_accuracy: 0.5825 - val_loss: 1.4926 - val_softmax_output_loss: 1.4961 - val_auxiliary1_output_loss: 1.5160 - val_auxiliary2_output_loss: 1.4645 - val_softmax_output_accuracy: 0.5434 - val_auxiliary1_output_accuracy: 0.5306 - val_auxiliary2_output_accuracy: 0.5442\n"
     ]
    }
   ],
   "source": [
    "history_inception = model_inception.fit(train_data, epochs=10, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = configure_data(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 12s 77ms/step - loss: 1.4902 - softmax_output_loss: 1.4851 - auxiliary1_output_loss: 1.5146 - auxiliary2_output_loss: 1.4725 - softmax_output_accuracy: 0.5390 - auxiliary1_output_accuracy: 0.5318 - auxiliary2_output_accuracy: 0.5419\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4901702404022217,\n",
       " 1.485058307647705,\n",
       " 1.5146241188049316,\n",
       " 1.4725314378738403,\n",
       " 0.5389999747276306,\n",
       " 0.5317999720573425,\n",
       " 0.5418999791145325]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inception.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's try transfer learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_resnet(X,y):\n",
    "    X = keras.applications.resnet50.preprocess_input(X)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(train_data,valid_data):\n",
    "    train_data = train_data.map(preprocess_resnet)\n",
    "    valid_data = valid_data.map(preprocess_resnet)\n",
    "    return train_data,valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,valid_data = preprocess_data(train_data,valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, None,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, None, None, 3 0           input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, None, None, 6 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, None, None, 6 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, None, None, 6 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, None, None, 6 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, None, None, 6 0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, None, None, 6 4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, None, None, 6 256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, None, None, 6 0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, None, None, 6 256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, None, None, 6 0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, None, None, 2 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, None, None, 2 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, None, None, 2 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, None, None, 6 256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, None, None, 6 0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, None, None, 6 256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, None, None, 6 0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, None, None, 2 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, None, None, 2 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, None, None, 6 16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, None, None, 6 256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, None, None, 6 0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, None, None, 6 36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, None, None, 6 256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, None, None, 6 0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, None, None, 2 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, None, None, 2 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, None, None, 2 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, None, None, 2 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, None, None, 1 32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, None, None, 1 512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, None, None, 1 0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, None, None, 1 512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, None, None, 1 0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, None, None, 5 131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, None, None, 5 0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, None, None, 5 0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, None, None, 1 512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, None, None, 1 0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, None, None, 1 512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, None, None, 1 0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, None, None, 5 0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, None, None, 5 0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, None, None, 1 512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, None, None, 1 0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, None, None, 1 512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, None, None, 1 0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, None, None, 5 0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, None, None, 5 0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, None, None, 1 65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, None, None, 1 512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, None, None, 1 0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, None, None, 1 147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, None, None, 1 512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, None, None, 1 0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, None, None, 5 66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, None, None, 5 2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, None, None, 5 0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, None, None, 5 0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, None, None, 2 131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, None, None, 2 0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, None, None, 2 0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, None, None, 1 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, None, None, 1 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, None, None, 1 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, None, None, 2 0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, None, None, 2 0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, None, None, 1 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, None, None, 1 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, None, None, 2 0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, None, None, 2 0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, None, None, 1 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, None, None, 1 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, None, None, 2 0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, None, None, 2 0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, None, None, 1 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, None, None, 1 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, None, None, 2 0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, None, None, 2 0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, None, None, 1 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, None, None, 1 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, None, None, 2 262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, None, None, 2 0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, None, None, 2 590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, None, None, 2 1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, None, None, 2 0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, None, None, 1 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, None, None, 1 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, None, None, 1 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, None, None, 1 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, None, None, 5 524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, None, None, 5 0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, None, None, 5 0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, None, None, 2 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, None, None, 2 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, None, None, 2 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, None, None, 5 0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, None, None, 5 0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, None, None, 2 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, None, None, 2 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, None, None, 5 1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, None, None, 5 0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, None, None, 5 2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, None, None, 5 2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, None, None, 5 0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, None, None, 2 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, None, None, 2 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, None, None, 2 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, None, None, 2 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 100)          204900      global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 23,792,612\n",
      "Trainable params: 23,739,492\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = keras.applications.resnet50.ResNet50(weights=\"imagenet\",include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "output = keras.layers.Dense(units=100, activation=\"softmax\")(avg)\n",
    "model = keras.Model(inputs=base_model.input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 315s 224ms/step - loss: 1.6207 - accuracy: 0.5529 - val_loss: 1.0155 - val_accuracy: 0.6698\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 308s 219ms/step - loss: 0.9396 - accuracy: 0.6923 - val_loss: 0.8982 - val_accuracy: 0.7090\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 308s 219ms/step - loss: 0.8415 - accuracy: 0.7258 - val_loss: 0.8483 - val_accuracy: 0.7238\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 308s 219ms/step - loss: 0.7914 - accuracy: 0.7416 - val_loss: 0.8184 - val_accuracy: 0.7306\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 306s 217ms/step - loss: 0.7581 - accuracy: 0.7530 - val_loss: 0.8006 - val_accuracy: 0.7362\n"
     ]
    }
   ],
   "source": [
    "#Must train BN layer ,if not then accuracy will be <10% always\n",
    "for layer in base_model.layers: # or model.layers[:-1]: \n",
    "    if isinstance(layer, keras.layers.BatchNormalization):\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "\n",
    "optimizer = keras.optimizers.SGD(lr=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "history_pretrained_resnet = model.fit(train_data, epochs=5, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "pd.DataFrame(history_pretrained_resnet.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1) # set the vertical range to [0-1]\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because GPU-Memory not enough we'll save model and load it again with empty memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.models.save_model(model,\"pretrained_cifar100_resnet_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"pretrained_cifar100_resnet_v3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 40s 127ms/step - loss: 0.7974 - accuracy: 0.7385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7973619103431702, 0.7384999990463257]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = configure_data(test_data)\n",
    "test_data = test_data.map(preprocess_resnet)\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine tuning model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 477s 339ms/step - loss: 0.9330 - accuracy: 0.7022 - val_loss: 0.6205 - val_accuracy: 0.7956\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 483s 343ms/step - loss: 0.3707 - accuracy: 0.8835 - val_loss: 0.5645 - val_accuracy: 0.8206\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 483s 343ms/step - loss: 0.2439 - accuracy: 0.9292 - val_loss: 0.5589 - val_accuracy: 0.8262\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 457s 325ms/step - loss: 0.1803 - accuracy: 0.9507 - val_loss: 0.5568 - val_accuracy: 0.8278\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 454s 323ms/step - loss: 0.1449 - accuracy: 0.9638 - val_loss: 0.5588 - val_accuracy: 0.8300\n"
     ]
    }
   ],
   "source": [
    "optimizer = keras.optimizers.SGD(lr=0.01, momentum=0.9, decay=0.01)\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer,metrics=[\"accuracy\"])\n",
    "history_pretrained_resnet = model.fit(train_data, epochs=5, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1407/1407 [==============================] - 437s 310ms/step - loss: 0.1182 - accuracy: 0.9734 - val_loss: 0.5675 - val_accuracy: 0.8294\n",
      "Epoch 2/15\n",
      "1407/1407 [==============================] - 438s 311ms/step - loss: 0.1021 - accuracy: 0.9781 - val_loss: 0.5681 - val_accuracy: 0.8318\n",
      "Epoch 3/15\n",
      "1407/1407 [==============================] - 439s 312ms/step - loss: 0.0864 - accuracy: 0.9834 - val_loss: 0.5737 - val_accuracy: 0.8334\n",
      "Epoch 4/15\n",
      "1407/1407 [==============================] - 439s 312ms/step - loss: 0.0768 - accuracy: 0.9868 - val_loss: 0.5763 - val_accuracy: 0.8332\n",
      "Epoch 5/15\n",
      "1407/1407 [==============================] - 439s 312ms/step - loss: 0.0691 - accuracy: 0.9890 - val_loss: 0.5839 - val_accuracy: 0.8336\n",
      "Epoch 6/15\n",
      "1407/1407 [==============================] - 439s 312ms/step - loss: 0.0642 - accuracy: 0.9898 - val_loss: 0.5829 - val_accuracy: 0.8334\n",
      "Epoch 7/15\n",
      "1407/1407 [==============================] - 439s 312ms/step - loss: 0.0589 - accuracy: 0.9922 - val_loss: 0.5891 - val_accuracy: 0.8342\n",
      "Epoch 8/15\n",
      "1407/1407 [==============================] - 450s 320ms/step - loss: 0.0551 - accuracy: 0.9922 - val_loss: 0.5909 - val_accuracy: 0.8324\n",
      "Epoch 9/15\n",
      "1407/1407 [==============================] - 547s 389ms/step - loss: 0.0518 - accuracy: 0.9933 - val_loss: 0.5944 - val_accuracy: 0.8332\n",
      "Epoch 10/15\n",
      "1407/1407 [==============================] - 562s 399ms/step - loss: 0.0476 - accuracy: 0.9946 - val_loss: 0.5961 - val_accuracy: 0.8338\n",
      "Epoch 11/15\n",
      "1407/1407 [==============================] - 443s 315ms/step - loss: 0.0463 - accuracy: 0.9946 - val_loss: 0.5987 - val_accuracy: 0.8318\n",
      "Epoch 12/15\n",
      "1407/1407 [==============================] - 422s 300ms/step - loss: 0.0442 - accuracy: 0.9947 - val_loss: 0.6018 - val_accuracy: 0.8336\n",
      "Epoch 13/15\n",
      "1407/1407 [==============================] - 422s 300ms/step - loss: 0.0419 - accuracy: 0.9958 - val_loss: 0.6029 - val_accuracy: 0.8344\n",
      "Epoch 14/15\n",
      "1407/1407 [==============================] - 422s 300ms/step - loss: 0.0398 - accuracy: 0.9959 - val_loss: 0.6053 - val_accuracy: 0.8342\n",
      "Epoch 15/15\n",
      "1407/1407 [==============================] - 421s 299ms/step - loss: 0.0384 - accuracy: 0.9962 - val_loss: 0.6065 - val_accuracy: 0.8338\n"
     ]
    }
   ],
   "source": [
    "#Continute training\n",
    "history_pretrained_resnet = model.fit(train_data, epochs=15, validation_data=valid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 39s 123ms/step - loss: 0.6628 - accuracy: 0.8214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6628488302230835, 0.821399986743927]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = configure_data(test_data)\n",
    "test_data = test_data.map(preprocess_resnet)\n",
    "model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
